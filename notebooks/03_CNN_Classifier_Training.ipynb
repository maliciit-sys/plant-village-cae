{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: CNN Classifier Training (Transfer Learning)\n",
    "\n",
    "**Project:** DeepSpec-Tomato: A Dual-Stage CAE-CNN Diagnostic Framework  \n",
    "**Author:** Muhammad Ali Tahir  \n",
    "**Date:** 2025  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook Objectives\n",
    "\n",
    "1. **Load pre-trained encoder** from CAE (Notebook 3)\n",
    "2. **Build classification head** ‚Äî Dense layers for 10-class classification\n",
    "3. **Two-phase training** ‚Äî Frozen encoder ‚Üí Fine-tuning\n",
    "4. **Comprehensive metrics** ‚Äî Accuracy, Precision, Recall, F1 per epoch\n",
    "5. **Best practices** ‚Äî Early stopping, LR scheduling, checkpointing on F1\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Transfer Learning Strategy\n",
    "\n",
    "**Phase 1 (Frozen Encoder):** Train only the classification head while keeping encoder weights fixed. This prevents catastrophic forgetting of learned features.\n",
    "\n",
    "**Phase 2 (Fine-tuning):** Unfreeze encoder and train end-to-end with a lower learning rate. This allows the encoder to adapt features specifically for classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Professional metrics\n",
    "try:\n",
    "    from torchmetrics.classification import (\n",
    "        MulticlassAccuracy, MulticlassPrecision,\n",
    "        MulticlassRecall, MulticlassF1Score\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Installing torchmetrics...\")\n",
    "    !pip install torchmetrics -q\n",
    "    from torchmetrics.classification import (\n",
    "        MulticlassAccuracy, MulticlassPrecision,\n",
    "        MulticlassRecall, MulticlassF1Score\n",
    "    )\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Notebook executed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD CONFIGURATION FROM PREVIOUS NOTEBOOKS\n",
    "# =============================================================================\n",
    "\n",
    "class ProjectPaths:\n",
    "    \"\"\"Centralized path management.\"\"\"\n",
    "    \n",
    "    ROOT = Path(\"/home/maliciit/ml-projects/python-projects/plant-village-cae\")\n",
    "    DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "    DATA_PROCESSED = ROOT / \"data\" / \"processed\"\n",
    "    NOTEBOOKS = ROOT / \"notebooks\"\n",
    "    MODELS = ROOT / \"models\"\n",
    "    OUTPUTS = ROOT / \"outputs\"\n",
    "    CHECKPOINTS = ROOT / \"checkpoints\"\n",
    "    LOGS = ROOT / \"logs\"\n",
    "    CONFIG = ROOT / \"config\"\n",
    "\n",
    "PATHS = ProjectPaths()\n",
    "\n",
    "# Load split metadata (from Notebook 2)\n",
    "metadata_path = PATHS.CONFIG / 'split_metadata.json'\n",
    "with open(metadata_path, 'r') as f:\n",
    "    METADATA = json.load(f)\n",
    "\n",
    "# Load CAE training summary (from Notebook 3)\n",
    "cae_summary_path = PATHS.CONFIG / 'cae_training_summary.json'\n",
    "with open(cae_summary_path, 'r') as f:\n",
    "    CAE_SUMMARY = json.load(f)\n",
    "\n",
    "# Extract configuration\n",
    "NORM_MEAN = METADATA['normalization']['mean']\n",
    "NORM_STD = METADATA['normalization']['std']\n",
    "IMAGE_SIZE = METADATA['image_config']['size']\n",
    "NUM_CLASSES = METADATA['classes']['num_classes']\n",
    "CLASS_NAMES_RAW = METADATA['classes']['names_raw']\n",
    "CLASS_NAMES = METADATA['classes']['names_display']\n",
    "\n",
    "print(f\"‚úì Split metadata loaded\")\n",
    "print(f\"‚úì CAE summary loaded (Best SSIM: {CAE_SUMMARY['best_ssim']:.4f})\")\n",
    "print(f\"\\n  Image size: {IMAGE_SIZE}√ó{IMAGE_SIZE}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"  Normalization mean: {[f'{m:.4f}' for m in NORM_MEAN]}\")\n",
    "print(f\"  Normalization std:  {[f'{s:.4f}' for s in NORM_STD]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG_CLF = {\n",
    "    # Data\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    # Phase 1: Frozen encoder\n",
    "    'phase1_epochs': 15,\n",
    "    'phase1_lr': 1e-3,\n",
    "    \n",
    "    # Phase 2: Fine-tuning (unfrozen)\n",
    "    'phase2_epochs': 25,\n",
    "    'phase2_lr': 1e-4,  # Lower LR for fine-tuning\n",
    "    \n",
    "    # Regularization\n",
    "    'weight_decay': 1e-4,\n",
    "    'dropout': 0.4,\n",
    "    \n",
    "    # Early stopping & scheduling\n",
    "    'patience': 7,\n",
    "    'lr_patience': 3,\n",
    "    'lr_factor': 0.5,\n",
    "    'min_lr': 1e-6,\n",
    "    \n",
    "    # Architecture\n",
    "    'hidden_dim': 512,\n",
    "    'latent_channels': CAE_SUMMARY['config']['latent_channels'],  # 128 from CAE\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "TRAIN_PATH = PATHS.DATA_PROCESSED / 'train'\n",
    "VAL_PATH = PATHS.DATA_PROCESSED / 'val'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFIER TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device:              {DEVICE}\")\n",
    "print(f\"Batch size:          {CONFIG_CLF['batch_size']}\")\n",
    "print(f\"\\nPhase 1 (Frozen):\")\n",
    "print(f\"  Epochs:            {CONFIG_CLF['phase1_epochs']}\")\n",
    "print(f\"  Learning rate:     {CONFIG_CLF['phase1_lr']}\")\n",
    "print(f\"\\nPhase 2 (Fine-tune):\")\n",
    "print(f\"  Epochs:            {CONFIG_CLF['phase2_epochs']}\")\n",
    "print(f\"  Learning rate:     {CONFIG_CLF['phase2_lr']}\")\n",
    "print(f\"\\nRegularization:\")\n",
    "print(f\"  Weight decay:      {CONFIG_CLF['weight_decay']}\")\n",
    "print(f\"  Dropout:           {CONFIG_CLF['dropout']}\")\n",
    "print(f\"  Early stop patience: {CONFIG_CLF['patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA TRANSFORMS WITH PROPER NORMALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Training: Augmentation + Normalization (from training set statistics)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD)  # From training set!\n",
    "])\n",
    "\n",
    "# Validation: Only resize + normalize (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(TRAIN_PATH, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(VAL_PATH, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG_CLF['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG_CLF['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training samples:   {len(train_dataset):,}\")\n",
    "print(f\"‚úì Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"‚úì Training batches:   {len(train_loader):,}\")\n",
    "print(f\"‚úì Validation batches: {len(val_loader):,}\")\n",
    "print(f\"\\n‚úì Using normalization from TRAINING SET (no data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENCODER ARCHITECTURE (MUST MATCH CAE)\n",
    "# =============================================================================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder architecture matching the CAE from Notebook 3.\n",
    "    \n",
    "    Input:  [B, 3, 128, 128]\n",
    "    Output: [B, 128, 16, 16]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # Block 1: 128√ó128√ó3 ‚Üí 64√ó64√ó32\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 2: 64√ó64√ó32 ‚Üí 32√ó32√ó64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 3: 32√ó32√ó64 ‚Üí 16√ó16√ó128\n",
    "            nn.Conv2d(64, latent_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(latent_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLASSIFIER ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class TomatoClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification model using pre-trained CAE encoder.\n",
    "    \n",
    "    Architecture:\n",
    "        - Pre-trained encoder (from CAE)\n",
    "        - Global Average Pooling (optional, more robust)\n",
    "        - Classification head with dropout\n",
    "    \n",
    "    Input:  [B, 3, 128, 128]\n",
    "    Output: [B, num_classes]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, num_classes, hidden_dim=512, dropout=0.4, use_gap=False):\n",
    "        super(TomatoClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.use_gap = use_gap\n",
    "        \n",
    "        if use_gap:\n",
    "            # Global Average Pooling approach (more robust, fewer params)\n",
    "            self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(128, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            # Flatten approach (matches your original)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(128 * 16 * 16, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        if self.use_gap:\n",
    "            features = self.gap(features)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        \"\"\"Freeze encoder weights for Phase 1 training.\"\"\"\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"‚úì Encoder frozen\")\n",
    "    \n",
    "    def unfreeze_encoder(self):\n",
    "        \"\"\"Unfreeze encoder weights for Phase 2 fine-tuning.\"\"\"\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"‚úì Encoder unfrozen\")\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Return count of trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD PRE-TRAINED ENCODER AND BUILD CLASSIFIER\n",
    "# =============================================================================\n",
    "\n",
    "# Create encoder\n",
    "encoder = Encoder(latent_channels=CONFIG_CLF['latent_channels'])\n",
    "\n",
    "# Load pre-trained weights from CAE\n",
    "encoder_path = PATHS.MODELS / 'cae_encoder.pth'\n",
    "checkpoint = torch.load(encoder_path, map_location=DEVICE)\n",
    "\n",
    "# Load encoder state dict\n",
    "encoder.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "print(f\"‚úì Loaded pre-trained encoder from: {encoder_path}\")\n",
    "print(f\"  CAE trained for {checkpoint['training_epochs']} epochs\")\n",
    "print(f\"  CAE best SSIM: {checkpoint['best_ssim']:.4f}\")\n",
    "print(f\"  CAE best PSNR: {checkpoint['best_psnr']:.2f} dB\")\n",
    "\n",
    "# Build classifier\n",
    "model = TomatoClassifier(\n",
    "    encoder=encoder,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    hidden_dim=CONFIG_CLF['hidden_dim'],\n",
    "    dropout=CONFIG_CLF['dropout'],\n",
    "    use_gap=False  # Set to True for GAP approach\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print architecture\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFIER ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PARAMETER COUNT\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Encoder parameters:    {encoder_params:,}\")\n",
    "print(f\"Classifier parameters: {classifier_params:,}\")\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loss: CrossEntropyLoss\n",
      "‚úì Metrics: Accuracy, Precision, Recall, F1 (Macro)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOSS FUNCTION AND METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Metrics (torchmetrics)\n",
    "metrics = {\n",
    "    'accuracy': MulticlassAccuracy(num_classes=NUM_CLASSES).to(DEVICE),\n",
    "    'precision': MulticlassPrecision(num_classes=NUM_CLASSES, average='macro').to(DEVICE),\n",
    "    'recall': MulticlassRecall(num_classes=NUM_CLASSES, average='macro').to(DEVICE),\n",
    "    'f1': MulticlassF1Score(num_classes=NUM_CLASSES, average='macro').to(DEVICE),\n",
    "}\n",
    "\n",
    "print(\"‚úì Loss: CrossEntropyLoss\")\n",
    "print(\"‚úì Metrics: Accuracy, Precision, Recall, F1 (Macro)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HISTORY TRACKER\n",
    "# =============================================================================\n",
    "\n",
    "class ClassifierHistory:\n",
    "    \"\"\"Track all training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'phase': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'learning_rate': [],\n",
    "            'epoch_time': [],\n",
    "        }\n",
    "        self.best_f1 = 0\n",
    "        self.best_accuracy = 0\n",
    "        self.best_epoch = 0\n",
    "        self.total_time = 0\n",
    "    \n",
    "    def update(self, epoch, phase, train_loss, val_loss, acc, prec, rec, f1, lr, time):\n",
    "        self.history['epoch'].append(epoch)\n",
    "        self.history['phase'].append(phase)\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['accuracy'].append(acc)\n",
    "        self.history['precision'].append(prec)\n",
    "        self.history['recall'].append(rec)\n",
    "        self.history['f1'].append(f1)\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        self.history['epoch_time'].append(time)\n",
    "        self.total_time += time\n",
    "        \n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_epoch = epoch\n",
    "        if acc > self.best_accuracy:\n",
    "            self.best_accuracy = acc\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return pd.DataFrame(self.history)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        df = self.get_dataframe()\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"‚úì Training history saved: {filepath}\")\n",
    "\n",
    "# Initialize\n",
    "history = ClassifierHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING AND VALIDATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, metrics, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = metrics['accuracy'](all_preds, all_labels).item()\n",
    "    prec = metrics['precision'](all_preds, all_labels).item()\n",
    "    rec = metrics['recall'](all_preds, all_labels).item()\n",
    "    f1 = metrics['f1'](all_preds, all_labels).item()\n",
    "    \n",
    "    return running_loss / len(loader), acc, prec, rec, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Loop (Two-Phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: FROZEN ENCODER (Train only classifier head)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: TRAINING WITH FROZEN ENCODER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Freeze encoder\n",
    "model.freeze_encoder()\n",
    "print(f\"Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# Optimizer for Phase 1 (only classifier parameters)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG_CLF['phase1_lr'],\n",
    "    weight_decay=CONFIG_CLF['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=CONFIG_CLF['lr_factor'],\n",
    "    patience=CONFIG_CLF['lr_patience'], min_lr=CONFIG_CLF['min_lr']\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "best_f1 = 0\n",
    "early_stop_counter = 0\n",
    "checkpoint_path = PATHS.CHECKPOINTS / 'classifier_best.pth'\n",
    "\n",
    "print(f\"\\nStarting Phase 1 training ({CONFIG_CLF['phase1_epochs']} epochs)...\\n\")\n",
    "\n",
    "for epoch in range(1, CONFIG_CLF['phase1_epochs'] + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, acc, prec, rec, f1, _, _ = validate_epoch(\n",
    "        model, val_loader, criterion, metrics, DEVICE\n",
    "    )\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history.update(epoch, 'frozen', train_loss, val_loss, acc, prec, rec, f1, current_lr, epoch_time)\n",
    "    \n",
    "    # Update scheduler (based on F1)\n",
    "    scheduler.step(f1)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch:02d}/{CONFIG_CLF['phase1_epochs']}] \"\n",
    "          f\"Loss: {val_loss:.4f} | \"\n",
    "          f\"Acc: {acc:.2%} | \"\n",
    "          f\"F1: {f1:.4f} | \"\n",
    "          f\"Prec: {prec:.4f} | \"\n",
    "          f\"Rec: {rec:.4f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Checkpoint on best F1\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        early_stop_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'phase': 'frozen',\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'f1': f1,\n",
    "            'accuracy': acc,\n",
    "            'config': CONFIG_CLF\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  ‚îî‚îÄ ‚úì Checkpoint saved (F1: {f1:.4f})\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"  ‚îî‚îÄ No improvement ({early_stop_counter}/{CONFIG_CLF['patience']})\")\n",
    "        \n",
    "        if early_stop_counter >= CONFIG_CLF['patience']:\n",
    "            print(f\"\\n[INFO] Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nPhase 1 complete. Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: FINE-TUNING (Unfreeze encoder)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: FINE-TUNING WITH UNFROZEN ENCODER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best checkpoint from Phase 1\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úì Loaded best Phase 1 model (F1: {checkpoint['f1']:.4f})\")\n",
    "\n",
    "# Unfreeze encoder\n",
    "model.unfreeze_encoder()\n",
    "print(f\"Trainable parameters: {model.get_trainable_params():,}\")\n",
    "\n",
    "# New optimizer with lower LR for fine-tuning\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG_CLF['phase2_lr'],\n",
    "    weight_decay=CONFIG_CLF['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=CONFIG_CLF['lr_factor'],\n",
    "    patience=CONFIG_CLF['lr_patience'], min_lr=CONFIG_CLF['min_lr']\n",
    ")\n",
    "\n",
    "# Reset early stopping\n",
    "early_stop_counter = 0\n",
    "phase1_epochs = len([e for e in history.history['phase'] if e == 'frozen'])\n",
    "\n",
    "print(f\"\\nStarting Phase 2 training ({CONFIG_CLF['phase2_epochs']} epochs)...\\n\")\n",
    "\n",
    "for epoch in range(1, CONFIG_CLF['phase2_epochs'] + 1):\n",
    "    global_epoch = phase1_epochs + epoch\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, acc, prec, rec, f1, _, _ = validate_epoch(\n",
    "        model, val_loader, criterion, metrics, DEVICE\n",
    "    )\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Update history\n",
    "    history.update(global_epoch, 'finetune', train_loss, val_loss, acc, prec, rec, f1, current_lr, epoch_time)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(f1)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch:02d}/{CONFIG_CLF['phase2_epochs']}] (Global: {global_epoch}) \"\n",
    "          f\"Loss: {val_loss:.4f} | \"\n",
    "          f\"Acc: {acc:.2%} | \"\n",
    "          f\"F1: {f1:.4f} | \"\n",
    "          f\"Prec: {prec:.4f} | \"\n",
    "          f\"Rec: {rec:.4f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Checkpoint on best F1\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        early_stop_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': global_epoch,\n",
    "            'phase': 'finetune',\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'f1': f1,\n",
    "            'accuracy': acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'config': CONFIG_CLF\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  ‚îî‚îÄ ‚úì Checkpoint saved (F1: {f1:.4f})\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"  ‚îî‚îÄ No improvement ({early_stop_counter}/{CONFIG_CLF['patience']})\")\n",
    "        \n",
    "        if early_stop_counter >= CONFIG_CLF['patience']:\n",
    "            print(f\"\\n[INFO] Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Training complete\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total epochs:        {len(history.history['epoch'])}\")\n",
    "print(f\"Total training time: {timedelta(seconds=int(history.total_time))}\")\n",
    "print(f\"Best epoch:          {history.best_epoch}\")\n",
    "print(f\"Best F1 Score:       {history.best_f1:.4f}\")\n",
    "print(f\"Best Accuracy:       {history.best_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT TRAINING CURVES\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs = history.history['epoch']\n",
    "phase1_end = len([p for p in history.history['phase'] if p == 'frozen'])\n",
    "\n",
    "# Plot 1: Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(epochs, history.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, history.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.axvline(x=phase1_end, color='green', linestyle='--', alpha=0.7, label='Phase 2 Start')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Convergence', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy & F1\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs, history.history['accuracy'], 'b-', label='Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "ax2.plot(epochs, history.history['f1'], 'g-', label='F1 Score', linewidth=2, marker='s', markersize=4)\n",
    "ax2.axvline(x=phase1_end, color='orange', linestyle='--', alpha=0.7, label='Phase 2 Start')\n",
    "ax2.axhline(y=0.99, color='red', linestyle=':', alpha=0.5, label='99% Target')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Accuracy & F1 Score', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0.5, 1.0)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Precision & Recall\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(epochs, history.history['precision'], 'purple', label='Precision', linewidth=2)\n",
    "ax3.plot(epochs, history.history['recall'], 'orange', label='Recall', linewidth=2)\n",
    "ax3.axvline(x=phase1_end, color='green', linestyle='--', alpha=0.7, label='Phase 2 Start')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Precision & Recall', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0.5, 1.0)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning Rate\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(epochs, history.history['learning_rate'], 'teal', linewidth=2)\n",
    "ax4.axvline(x=phase1_end, color='green', linestyle='--', alpha=0.7, label='Phase 2 Start')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "ax4.set_yscale('log')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Classifier Training Progress (Two-Phase)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "fig_path = PATHS.OUTPUTS / 'fig_14_classifier_training_curves.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úì Figure saved: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded best model from epoch 39 (Phase: finetune)\n",
      "  F1 Score: 0.9774\n",
      "  Accuracy: 97.63%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD BEST MODEL\n",
    "# =============================================================================\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']} (Phase: {checkpoint['phase']})\")\n",
    "print(f\"  F1 Score: {checkpoint['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {checkpoint['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFUSION MATRIX ON VALIDATION SET\n",
    "# =============================================================================\n",
    "\n",
    "# Get all predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted', fontsize=11)\n",
    "axes[0].set_ylabel('True', fontsize=11)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=12, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted', fontsize=11)\n",
    "axes[1].set_ylabel('True', fontsize=11)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "fig_path = PATHS.OUTPUTS / 'fig_15_confusion_matrix.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úì Figure saved: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT (Validation Set)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=4)\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_dict = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_path = PATHS.LOGS / 'classification_report_val.csv'\n",
    "report_df.to_csv(report_path)\n",
    "print(f\"\\n‚úì Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Save Final Model & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Final model saved: /home/maliciit/ml-projects/python-projects/plant-village-cae/models/classifier_final.pth\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Save full model for inference\n",
    "final_model_path = PATHS.MODELS / 'classifier_final.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'class_names_raw': CLASS_NAMES_RAW,\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'normalization': {'mean': NORM_MEAN, 'std': NORM_STD},\n",
    "    'best_f1': history.best_f1,\n",
    "    'best_accuracy': history.best_accuracy,\n",
    "    'config': CONFIG_CLF\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"‚úì Final model saved: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE TRAINING HISTORY & SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "# Save history\n",
    "history_path = PATHS.LOGS / 'classifier_training_history.csv'\n",
    "history.save(history_path)\n",
    "\n",
    "# Training summary\n",
    "training_summary = {\n",
    "    'completed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_epochs': len(history.history['epoch']),\n",
    "    'total_training_time_seconds': history.total_time,\n",
    "    'total_training_time_formatted': str(timedelta(seconds=int(history.total_time))),\n",
    "    'best_epoch': history.best_epoch,\n",
    "    'best_f1': history.best_f1,\n",
    "    'best_accuracy': history.best_accuracy,\n",
    "    'phase1_epochs': len([p for p in history.history['phase'] if p == 'frozen']),\n",
    "    'phase2_epochs': len([p for p in history.history['phase'] if p == 'finetune']),\n",
    "    'config': CONFIG_CLF,\n",
    "    'model_paths': {\n",
    "        'final_model': str(final_model_path),\n",
    "        'checkpoint': str(checkpoint_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = PATHS.CONFIG / 'classifier_training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Training summary saved: {summary_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(training_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Completed in This Notebook\n",
    "\n",
    "1. **Loaded pre-trained encoder** from CAE (Notebook 3)\n",
    "2. **Two-phase training:**\n",
    "   - Phase 1: Frozen encoder (train classifier head only)\n",
    "   - Phase 2: Fine-tuning (end-to-end with lower LR)\n",
    "3. **Comprehensive metrics:** Accuracy, Precision, Recall, F1 per epoch\n",
    "4. **Confusion matrix & classification report**\n",
    "5. **Model export** for inference\n",
    "\n",
    "### üìä Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Best F1 Score | See summary above |\n",
    "| Best Accuracy | See summary above |\n",
    "| Training Time | See summary above |\n",
    "\n",
    "### üìÅ Files Created\n",
    "\n",
    "- `models/classifier_final.pth` ‚Äî Final model for inference\n",
    "- `checkpoints/classifier_best.pth` ‚Äî Best checkpoint\n",
    "- `logs/classifier_training_history.csv` ‚Äî Epoch metrics\n",
    "- `logs/classification_report_val.csv` ‚Äî Per-class metrics\n",
    "- `config/classifier_training_summary.json` ‚Äî Training config & results\n",
    "\n",
    "### ‚û°Ô∏è Next Notebook: `5_Threshold_Optimization_and_Evaluation.ipynb`\n",
    "\n",
    "- Evaluate on TEST set (held-out data)\n",
    "- Threshold optimization for precision/recall trade-off\n",
    "- t-SNE visualization of learned features\n",
    "- Final scientific figures for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK COMPLETION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì NOTEBOOK 4 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Execution finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nBest F1 Score: {history.best_f1:.4f}\")\n",
    "print(f\"Best Accuracy: {history.best_accuracy:.2%}\")\n",
    "print(\"\\nProceed to: 5_Threshold_Optimization_and_Evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

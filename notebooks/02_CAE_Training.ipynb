{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Convolutional Autoencoder (CAE) Training\n",
    "\n",
    "**Project:** DeepSpec-Tomato: A Dual-Stage CAE-CNN Diagnostic Framework  \n",
    "**Author:** Muhammad Ali Tahir  \n",
    "**Date:** 2025  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook Objectives\n",
    "\n",
    "1. **Self-supervised pre-training** ‚Äî Learn latent representations of tomato leaves\n",
    "2. **Reconstruction learning** ‚Äî Minimize MSE while maximizing structural fidelity\n",
    "3. **Comprehensive metric tracking** ‚Äî Loss, PSNR, SSIM per epoch with timing\n",
    "4. **Best practices** ‚Äî Early stopping, LR scheduling, checkpointing\n",
    "5. **Scientific visualization** ‚Äî Convergence curves, reconstruction quality\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Scientific Rationale\n",
    "\n",
    "The CAE learns to compress leaf images into a latent \"spectral fingerprint\" and reconstruct them.\n",
    "This forces the encoder to capture:\n",
    "- **Vein structures** ‚Äî Critical for identifying vascular diseases\n",
    "- **Texture patterns** ‚Äî Distinguishes fungal spots from bacterial lesions\n",
    "- **Color distributions** ‚Äî Chlorophyll degradation signatures\n",
    "\n",
    "The pre-trained encoder will be transferred to the classifier in Notebook 4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Academic metrics\n",
    "try:\n",
    "    from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "except ImportError:\n",
    "    print(\"Installing torchmetrics...\")\n",
    "    !pip install torchmetrics -q\n",
    "    from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Notebook executed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD CONFIGURATION FROM NOTEBOOK 2\n",
    "# =============================================================================\n",
    "\n",
    "class ProjectPaths:\n",
    "    \"\"\"Centralized path management for the entire project.\"\"\"\n",
    "    \n",
    "    ROOT = Path(\"/home/maliciit/ml-projects/python-projects/plant-village-cae\")\n",
    "    \n",
    "    # Data paths\n",
    "    DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "    DATA_PROCESSED = ROOT / \"data\" / \"processed\"\n",
    "    \n",
    "    # Output paths\n",
    "    NOTEBOOKS = ROOT / \"notebooks\"\n",
    "    MODELS = ROOT / \"models\"\n",
    "    OUTPUTS = ROOT / \"outputs\"\n",
    "    CHECKPOINTS = ROOT / \"checkpoints\"\n",
    "    LOGS = ROOT / \"logs\"\n",
    "    CONFIG = ROOT / \"config\"\n",
    "\n",
    "PATHS = ProjectPaths()\n",
    "\n",
    "# Load split metadata from Notebook 2\n",
    "metadata_path = PATHS.CONFIG / 'split_metadata.json'\n",
    "\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        METADATA = json.load(f)\n",
    "    print(f\"‚úì Metadata loaded from: {metadata_path}\")\n",
    "    \n",
    "    # Extract key configuration\n",
    "    NORM_MEAN = METADATA['normalization']['mean']\n",
    "    NORM_STD = METADATA['normalization']['std']\n",
    "    IMAGE_SIZE = METADATA['image_config']['size']\n",
    "    NUM_CLASSES = METADATA['classes']['num_classes']\n",
    "    CLASS_NAMES = METADATA['classes']['names_display']\n",
    "    \n",
    "    print(f\"\\n  Image size: {IMAGE_SIZE}√ó{IMAGE_SIZE}\")\n",
    "    print(f\"  Normalization mean: {[f'{m:.4f}' for m in NORM_MEAN]}\")\n",
    "    print(f\"  Normalization std:  {[f'{s:.4f}' for s in NORM_STD]}\")\n",
    "    print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Metadata not found at {metadata_path}. \"\n",
    "        \"Please run Notebook 2 first.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG_CAE = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 7,              # Early stopping patience\n",
    "    'lr_patience': 3,           # LR scheduler patience\n",
    "    'lr_factor': 0.5,           # LR reduction factor\n",
    "    'min_lr': 1e-6,             # Minimum learning rate\n",
    "    'latent_channels': 128,     # Bottleneck channels\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "TRAIN_PATH = PATHS.DATA_PROCESSED / 'train'\n",
    "VAL_PATH = PATHS.DATA_PROCESSED / 'val'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CAE TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device:              {DEVICE}\")\n",
    "print(f\"Batch size:          {CONFIG_CAE['batch_size']}\")\n",
    "print(f\"Max epochs:          {CONFIG_CAE['epochs']}\")\n",
    "print(f\"Learning rate:       {CONFIG_CAE['learning_rate']}\")\n",
    "print(f\"Weight decay:        {CONFIG_CAE['weight_decay']}\")\n",
    "print(f\"Early stop patience: {CONFIG_CAE['patience']}\")\n",
    "print(f\"LR scheduler:        ReduceLROnPlateau (patience={CONFIG_CAE['lr_patience']}, factor={CONFIG_CAE['lr_factor']})\")\n",
    "print(f\"Latent channels:     {CONFIG_CAE['latent_channels']}\")\n",
    "print(f\"\\nTrain path: {TRAIN_PATH}\")\n",
    "print(f\"Val path:   {VAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA TRANSFORMS\n",
    "# =============================================================================\n",
    "\n",
    "# For CAE: We do NOT normalize (keep pixels in [0,1] for reconstruction)\n",
    "# Normalization would make reconstruction targets harder to interpret\n",
    "# We'll normalize only for the classifier in Notebook 4\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(TRAIN_PATH, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(VAL_PATH, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG_CAE['batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG_CAE['batch_size'], \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training samples:   {len(train_dataset):,}\")\n",
    "print(f\"‚úì Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"‚úì Training batches:   {len(train_loader):,}\")\n",
    "print(f\"‚úì Validation batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE SAMPLE BATCH\n",
    "# =============================================================================\n",
    "\n",
    "def show_batch(loader, n_images=8):\n",
    "    \"\"\"Display a batch of images from the data loader.\"\"\"\n",
    "    images, labels = next(iter(loader))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 2, 2.5))\n",
    "    \n",
    "    for i in range(min(n_images, len(images))):\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(CLASS_NAMES[labels[i]][:12], fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Sample Training Batch', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. CAE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONVOLUTIONAL AUTOENCODER ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class ConvolutionalAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Autoencoder for self-supervised feature learning.\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: 3 Conv blocks (3‚Üí32‚Üí64‚Üí128) with stride-2 downsampling\n",
    "        Bottleneck: 16√ó16√ó128 latent representation\n",
    "        Decoder: 3 ConvTranspose blocks to reconstruct 128√ó128√ó3\n",
    "    \n",
    "    Input:  [B, 3, 128, 128]\n",
    "    Latent: [B, 128, 16, 16]\n",
    "    Output: [B, 3, 128, 128]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder: Compress spatial information into latent bottleneck\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Block 1: 128√ó128√ó3 ‚Üí 64√ó64√ó32\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 2: 64√ó64√ó32 ‚Üí 32√ó32√ó64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 3: 32√ó32√ó64 ‚Üí 16√ó16√ó128\n",
    "            nn.Conv2d(64, latent_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(latent_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Decoder: Reconstruct from latent representation\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Block 1: 16√ó16√ó128 ‚Üí 32√ó32√ó64\n",
    "            nn.ConvTranspose2d(latent_channels, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 2: 32√ó32√ó64 ‚Üí 64√ó64√ó32\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 3: 64√ó64√ó32 ‚Üí 128√ó128√ó3\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output in [0, 1] to match input range\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent representation.\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation to reconstruction.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: encode then decode.\"\"\"\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "# Instantiate model\n",
    "model = ConvolutionalAutoencoder(latent_channels=CONFIG_CAE['latent_channels']).to(DEVICE)\n",
    "\n",
    "# Print architecture summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CAE ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "decoder_params = sum(p.numel() for p in model.decoder.parameters())\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PARAMETER COUNT\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Encoder parameters:  {encoder_params:,}\")\n",
    "print(f\"Decoder parameters:  {decoder_params:,}\")\n",
    "print(f\"Total parameters:    {total_params:,}\")\n",
    "print(f\"Trainable:           {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFY ARCHITECTURE WITH DUMMY INPUT\n",
    "# =============================================================================\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "dummy_latent = model.encode(dummy_input)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARCHITECTURE VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input shape:   {list(dummy_input.shape)}\")\n",
    "print(f\"Latent shape:  {list(dummy_latent.shape)}  ‚Üê Bottleneck (spectral fingerprint)\")\n",
    "print(f\"Output shape:  {list(dummy_output.shape)}\")\n",
    "print(f\"\\nLatent dimensions: {dummy_latent.shape[1]} channels √ó {dummy_latent.shape[2]}√ó{dummy_latent.shape[3]} spatial\")\n",
    "print(f\"Compression ratio: {(3*128*128) / (dummy_latent.shape[1]*dummy_latent.shape[2]*dummy_latent.shape[3]):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOSS, OPTIMIZER, SCHEDULER, METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# Loss function: MSE for reconstruction\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Adam with weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG_CAE['learning_rate'],\n",
    "    weight_decay=CONFIG_CAE['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: Reduce on plateau\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=CONFIG_CAE['lr_factor'],\n",
    "    patience=CONFIG_CAE['lr_patience'],\n",
    "    min_lr=CONFIG_CAE['min_lr']\n",
    ")\n",
    "\n",
    "# Initialize professional metrics\n",
    "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(DEVICE)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(DEVICE)\n",
    "\n",
    "print(\"‚úì Loss function: MSELoss\")\n",
    "print(f\"‚úì Optimizer: Adam (lr={CONFIG_CAE['learning_rate']}, weight_decay={CONFIG_CAE['weight_decay']})\")\n",
    "print(f\"‚úì Scheduler: ReduceLROnPlateau (patience={CONFIG_CAE['lr_patience']}, factor={CONFIG_CAE['lr_factor']})\")\n",
    "print(\"‚úì Metrics: PSNR, SSIM (torchmetrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HISTORY TRACKER\n",
    "# =============================================================================\n",
    "\n",
    "class TrainingHistory:\n",
    "    \"\"\"Track and store all training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'psnr': [],\n",
    "            'ssim': [],\n",
    "            'learning_rate': [],\n",
    "            'epoch_time': [],\n",
    "        }\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_psnr = 0\n",
    "        self.best_ssim = 0\n",
    "        self.best_epoch = 0\n",
    "        self.total_time = 0\n",
    "    \n",
    "    def update(self, epoch, train_loss, val_loss, psnr, ssim, lr, epoch_time):\n",
    "        \"\"\"Update history with new epoch metrics.\"\"\"\n",
    "        self.history['epoch'].append(epoch)\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['psnr'].append(psnr)\n",
    "        self.history['ssim'].append(ssim)\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        self.history['epoch_time'].append(epoch_time)\n",
    "        self.total_time += epoch_time\n",
    "        \n",
    "        # Track best metrics\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "        if psnr > self.best_psnr:\n",
    "            self.best_psnr = psnr\n",
    "        if ssim > self.best_ssim:\n",
    "            self.best_ssim = ssim\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Return history as pandas DataFrame.\"\"\"\n",
    "        return pd.DataFrame(self.history)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save history to CSV.\"\"\"\n",
    "        df = self.get_dataframe()\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"‚úì Training history saved: {filepath}\")\n",
    "\n",
    "# Initialize history tracker\n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, psnr_metric, ssim_metric, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    running_ssim = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute metrics\n",
    "            running_loss += criterion(outputs, images).item()\n",
    "            running_psnr += psnr_metric(outputs, images).item()\n",
    "            running_ssim += ssim_metric(outputs, images).item()\n",
    "    \n",
    "    n_batches = len(loader)\n",
    "    return running_loss / n_batches, running_psnr / n_batches, running_ssim / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING CAE TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Epochs: {CONFIG_CAE['epochs']} (max)\")\n",
    "print(f\"Early stopping patience: {CONFIG_CAE['patience']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_path = PATHS.CHECKPOINTS / 'cae_best.pth'\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, CONFIG_CAE['epochs'] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, psnr, ssim = validate_epoch(\n",
    "        model, val_loader, criterion, psnr_metric, ssim_metric, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Update history\n",
    "    history.update(epoch, train_loss, val_loss, psnr, ssim, current_lr, epoch_time)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch:02d}/{CONFIG_CAE['epochs']}] \"\n",
    "          f\"Train Loss: {train_loss:.5f} | \"\n",
    "          f\"Val Loss: {val_loss:.5f} | \"\n",
    "          f\"PSNR: {psnr:.2f} dB | \"\n",
    "          f\"SSIM: {ssim:.4f} | \"\n",
    "          f\"LR: {current_lr:.2e} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Checkpointing (save best model)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'psnr': psnr,\n",
    "            'ssim': ssim,\n",
    "            'config': CONFIG_CAE\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"  ‚îî‚îÄ ‚úì Checkpoint saved (Val Loss: {val_loss:.5f})\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"  ‚îî‚îÄ No improvement ({early_stop_counter}/{CONFIG_CAE['patience']})\")\n",
    "        \n",
    "        if early_stop_counter >= CONFIG_CAE['patience']:\n",
    "            print(f\"\\n[INFO] Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Training complete\n",
    "total_training_time = time.time() - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total epochs:        {epoch}\")\n",
    "print(f\"Total training time: {timedelta(seconds=int(total_training_time))}\")\n",
    "print(f\"Best epoch:          {history.best_epoch}\")\n",
    "print(f\"Best val loss:       {history.best_val_loss:.5f}\")\n",
    "print(f\"Best PSNR:           {history.best_psnr:.2f} dB\")\n",
    "print(f\"Best SSIM:           {history.best_ssim:.4f}\")\n",
    "print(f\"Checkpoint saved:    {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOT TRAINING CURVES\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs = history.history['epoch']\n",
    "\n",
    "# Plot 1: Loss Convergence\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(epochs, history.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, history.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.axvline(x=history.best_epoch, color='green', linestyle='--', label=f'Best Epoch ({history.best_epoch})')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Loss Convergence', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: PSNR (Signal Quality)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs, history.history['psnr'], 'g-', linewidth=2)\n",
    "ax2.axhline(y=history.best_psnr, color='green', linestyle='--', alpha=0.5, label=f'Best: {history.best_psnr:.2f} dB')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('PSNR (dB)')\n",
    "ax2.set_title('Peak Signal-to-Noise Ratio', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: SSIM (Structural Similarity)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(epochs, history.history['ssim'], 'orange', linewidth=2)\n",
    "ax3.axhline(y=history.best_ssim, color='orange', linestyle='--', alpha=0.5, label=f'Best: {history.best_ssim:.4f}')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('SSIM')\n",
    "ax3.set_title('Structural Similarity Index', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning Rate\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(epochs, history.history['learning_rate'], 'purple', linewidth=2)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CAE Training Progress', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = PATHS.OUTPUTS / 'fig_11_cae_training_curves.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úì Figure saved: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Reconstruction Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD BEST MODEL AND VISUALIZE RECONSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"  Val Loss: {checkpoint['val_loss']:.5f}\")\n",
    "print(f\"  PSNR: {checkpoint['psnr']:.2f} dB\")\n",
    "print(f\"  SSIM: {checkpoint['ssim']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE RECONSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_reconstructions(model, loader, class_names, n_samples=8, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Visualize original vs reconstructed images with per-image SSIM.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructions = model(images)\n",
    "    \n",
    "    # Move to CPU for visualization\n",
    "    images_cpu = images.cpu()\n",
    "    recon_cpu = reconstructions.cpu()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(3, n_samples, figsize=(n_samples * 2.5, 8))\n",
    "    \n",
    "    for i in range(min(n_samples, len(images))):\n",
    "        # Original\n",
    "        orig_img = images_cpu[i].permute(1, 2, 0).numpy()\n",
    "        axes[0, i].imshow(orig_img)\n",
    "        axes[0, i].set_title(class_names[labels[i]][:15], fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstructed\n",
    "        recon_img = recon_cpu[i].permute(1, 2, 0).numpy()\n",
    "        axes[1, i].imshow(recon_img)\n",
    "        \n",
    "        # Compute per-image SSIM\n",
    "        with torch.no_grad():\n",
    "            img_ssim = ssim_metric(\n",
    "                reconstructions[i:i+1], \n",
    "                images[i:i+1]\n",
    "            ).item()\n",
    "        axes[1, i].set_title(f'SSIM: {img_ssim:.3f}', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Difference (amplified)\n",
    "        diff = np.abs(orig_img - recon_img)\n",
    "        diff_amplified = np.clip(diff * 5, 0, 1)  # Amplify for visibility\n",
    "        axes[2, i].imshow(diff_amplified)\n",
    "        axes[2, i].set_title('Diff (5x)', fontsize=9)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    # Row labels\n",
    "    axes[0, 0].set_ylabel('Original', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Reconstructed', fontsize=11, fontweight='bold')\n",
    "    axes[2, 0].set_ylabel('Difference', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('CAE Reconstruction Quality Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = PATHS.OUTPUTS / 'fig_12_cae_reconstructions.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"‚úì Figure saved: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_reconstructions(model, val_loader, CLASS_NAMES, n_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RECONSTRUCTION QUALITY BY CLASS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_class_reconstruction_quality(model, loader, class_names, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Compute average reconstruction metrics per class.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    class_ssim = {name: [] for name in class_names}\n",
    "    class_psnr = {name: [] for name in class_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Computing class metrics\"):\n",
    "            images = images.to(device)\n",
    "            reconstructions = model(images)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                class_name = class_names[labels[i]]\n",
    "                \n",
    "                # Per-image metrics\n",
    "                ssim_val = ssim_metric(reconstructions[i:i+1], images[i:i+1]).item()\n",
    "                psnr_val = psnr_metric(reconstructions[i:i+1], images[i:i+1]).item()\n",
    "                \n",
    "                class_ssim[class_name].append(ssim_val)\n",
    "                class_psnr[class_name].append(psnr_val)\n",
    "    \n",
    "    # Compute averages\n",
    "    results = []\n",
    "    for name in class_names:\n",
    "        results.append({\n",
    "            'class': name,\n",
    "            'avg_ssim': np.mean(class_ssim[name]),\n",
    "            'std_ssim': np.std(class_ssim[name]),\n",
    "            'avg_psnr': np.mean(class_psnr[name]),\n",
    "            'std_psnr': np.std(class_psnr[name]),\n",
    "            'n_samples': len(class_ssim[name])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute per-class metrics\n",
    "class_metrics = compute_class_reconstruction_quality(model, val_loader, CLASS_NAMES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECONSTRUCTION QUALITY BY CLASS\")\n",
    "print(\"=\"*80)\n",
    "print(class_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE CLASS-WISE METRICS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sort by SSIM\n",
    "class_metrics_sorted = class_metrics.sort_values('avg_ssim', ascending=True)\n",
    "\n",
    "# Plot 1: SSIM by class\n",
    "ax1 = axes[0]\n",
    "colors = ['#2ecc71' if 'Healthy' in name else '#3498db' for name in class_metrics_sorted['class']]\n",
    "bars1 = ax1.barh(class_metrics_sorted['class'], class_metrics_sorted['avg_ssim'], \n",
    "                 xerr=class_metrics_sorted['std_ssim'], color=colors, capsize=3)\n",
    "ax1.set_xlabel('SSIM', fontsize=11)\n",
    "ax1.set_title('Reconstruction SSIM by Class', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.axvline(x=class_metrics['avg_ssim'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {class_metrics[\"avg_ssim\"].mean():.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: PSNR by class\n",
    "ax2 = axes[1]\n",
    "class_metrics_sorted_psnr = class_metrics.sort_values('avg_psnr', ascending=True)\n",
    "colors2 = ['#2ecc71' if 'Healthy' in name else '#e74c3c' for name in class_metrics_sorted_psnr['class']]\n",
    "bars2 = ax2.barh(class_metrics_sorted_psnr['class'], class_metrics_sorted_psnr['avg_psnr'], \n",
    "                 xerr=class_metrics_sorted_psnr['std_psnr'], color=colors2, capsize=3)\n",
    "ax2.set_xlabel('PSNR (dB)', fontsize=11)\n",
    "ax2.set_title('Reconstruction PSNR by Class', fontsize=12, fontweight='bold')\n",
    "ax2.axvline(x=class_metrics['avg_psnr'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {class_metrics[\"avg_psnr\"].mean():.1f} dB')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "fig_path = PATHS.OUTPUTS / 'fig_13_cae_class_metrics.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "print(f\"‚úì Figure saved: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Save Final Model & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE ENCODER FOR CLASSIFIER (NOTEBOOK 4)\n",
    "# =============================================================================\n",
    "\n",
    "# Save just the encoder weights for transfer learning\n",
    "encoder_path = PATHS.MODELS / 'cae_encoder.pth'\n",
    "encoder_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': model.encoder.state_dict(),\n",
    "    'latent_channels': CONFIG_CAE['latent_channels'],\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'best_ssim': history.best_ssim,\n",
    "    'best_psnr': history.best_psnr,\n",
    "    'training_epochs': len(history.history['epoch'])\n",
    "}, encoder_path)\n",
    "\n",
    "print(f\"‚úì Encoder saved: {encoder_path}\")\n",
    "\n",
    "# Save full model for reconstruction tasks\n",
    "full_model_path = PATHS.MODELS / 'cae_full.pth'\n",
    "torch.save(model.state_dict(), full_model_path)\n",
    "print(f\"‚úì Full CAE saved: {full_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE TRAINING HISTORY\n",
    "# =============================================================================\n",
    "\n",
    "# Save as CSV\n",
    "history_path = PATHS.LOGS / 'cae_training_history.csv'\n",
    "history_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "history.save(history_path)\n",
    "\n",
    "# Save training summary as JSON\n",
    "training_summary = {\n",
    "    'completed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_epochs': len(history.history['epoch']),\n",
    "    'total_training_time_seconds': history.total_time,\n",
    "    'total_training_time_formatted': str(timedelta(seconds=int(history.total_time))),\n",
    "    'best_epoch': history.best_epoch,\n",
    "    'best_val_loss': history.best_val_loss,\n",
    "    'best_psnr': history.best_psnr,\n",
    "    'best_ssim': history.best_ssim,\n",
    "    'final_learning_rate': history.history['learning_rate'][-1],\n",
    "    'config': CONFIG_CAE,\n",
    "    'model_paths': {\n",
    "        'encoder': str(encoder_path),\n",
    "        'full_model': str(full_model_path),\n",
    "        'checkpoint': str(checkpoint_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = PATHS.CONFIG / 'cae_training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Training summary saved: {summary_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(training_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Completed in This Notebook\n",
    "\n",
    "1. **CAE architecture** ‚Äî 3-block encoder/decoder with 128-channel bottleneck\n",
    "2. **Self-supervised training** ‚Äî Reconstruction loss minimization\n",
    "3. **Metric tracking** ‚Äî Loss, PSNR, SSIM, LR, time per epoch\n",
    "4. **Best practices** ‚Äî Early stopping, LR scheduling, checkpointing\n",
    "5. **Quality analysis** ‚Äî Per-class reconstruction metrics\n",
    "6. **Export** ‚Äî Encoder weights for transfer learning\n",
    "\n",
    "### üìä Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Best Epoch | See summary above |\n",
    "| Best PSNR | See summary above |\n",
    "| Best SSIM | See summary above |\n",
    "| Training Time | See summary above |\n",
    "\n",
    "### üìÅ Files Created\n",
    "\n",
    "- `models/cae_encoder.pth` ‚Äî Encoder weights for classifier\n",
    "- `models/cae_full.pth` ‚Äî Full CAE model\n",
    "- `checkpoints/cae_best.pth` ‚Äî Best checkpoint with optimizer state\n",
    "- `logs/cae_training_history.csv` ‚Äî Epoch-by-epoch metrics\n",
    "- `config/cae_training_summary.json` ‚Äî Training configuration & results\n",
    "\n",
    "### ‚û°Ô∏è Next Notebook: `4_CNN_Classifier_Training.ipynb`\n",
    "\n",
    "- Load pre-trained encoder\n",
    "- Add classification head\n",
    "- Fine-tune on disease classification\n",
    "- Track F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK COMPLETION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì NOTEBOOK 3 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Execution finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nGenerated figures: {PATHS.OUTPUTS}\")\n",
    "print(f\"Model saved: {PATHS.MODELS}\")\n",
    "print(f\"Training logs: {PATHS.LOGS}\")\n",
    "print(\"\\nProceed to: 4_CNN_Classifier_Training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
